{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cda7bde-b393-4412-b4f7-12b9873f7fbc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Topic Modelling - Citation Prediction Project\n",
    "By Jakub Wujec and Jakub Å»mujdzin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad719e-c71c-4d83-842d-58a7160334e2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Abstract\n",
    "This project aims to develop an article citation prediction model for identifying and categorizing articles for use in topic modelling. We examine the text of the articles to identify common semantics and categories, and use machine learning techniques to build predictive models based on these observations. We then use our model to evaluate its accuracy in predicting citation count. Our proposed model will involve utilizing natural language processing (NLP) techniques such as topic modeling, document-term matrices to extract features from the articles and build predictive models. We will also used supervised tree bosting algorithm (XGBoost) to predict the citation score based on topics present in an article. The research mainly focuses on identifying topics which are more \"hot\" in case of citations, not particularly to correctly predict citation count of an article. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdaf98a-46f7-480e-afab-6ab44080fa98",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keywords\n",
    "Topic modelling, XGBoost, citation, regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab07c79-9807-47b8-9246-8008b08b868d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d54a600-5a24-49ff-b04c-0b748f943b99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "df = pd.read_json(Path.cwd() / \"final_df\" / \"final_df.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14b940-f394-4226-8314-ccb1abfa12c9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Research questions\n",
    "- How can the text content of a paper be used to predict its citation count?\n",
    "- Can machine learning algorithms be trained to accurately predict the citation count of a paper based on its text content?\n",
    "- Can combination of the tasks of topic modelling and citation prediction produce better results than when the tasks are separated?\n",
    "### Motivation standing for undertaking the topic\n",
    "The topic was undertaken due to 2 key factors. As students, with some interest in publishing papers in the future, maximizing citation score can be one of the objectives when writing some text. The first issue was to determine whether Machine Learning (Natural Language Processing) tools can be used to find correlation between certain topics and citation score. Furthermore, if such relationship is existent, it was key to find out which topics produce the highest citation score and on which to focus. Finally, it was key to determine, whether splitting the topic modelling and citation prediction tasks can produce better results. Thanks to that, it was easier to decide on which models to focus for accurate predictions in the future.\n",
    "### Methodology\n",
    "- Models\n",
    "\n",
    "Latent Dirichlet Allocation model was chosen as a basic tool for topic modelling in the task. It is due to its state-of-the-art results on different texts. The model is also currently perceived as among the best models created for Topic Modelling.\n",
    "Furthermore, for the task of predicting citation score based on certain topics, XGBoost, a gradient boosted trees method was chosen, along with sLDA, an iteration of LDA model for supervised tasks. XGBoost is considered among the supervised models that produce the highest accuracy predictions. sLDA, on the other hand, combines the choice of hyperparameters for topic modelling and regression, which theoretically could yield better results in terms of predicting citation score of a peper, since the tasks are not split.\n",
    "\n",
    "- Dataset source & presentation\n",
    "\n",
    "The dataset is available in final_df/final_df.json directory. \n",
    "To construct this dataset, we have used arXiv API and Google Scholar. Using arXiv API, we have searched with \"machine learning\" query to download article's titles, authors and links to PDF files containing the text. Then, we used BeautifulSoup to scrap Google Scholar. For each article in a dataframe, we have searched in Google Scholar for article's title and article's authors, then extracted citation count, if it was available. Finally, we used PyPDF2 to download PDF files from the links we have scrapped earlier, from arXiv. We have saved the articles to a json file, containing publication's title, text and citation score.\n",
    "Finally, we have arrived at 1234 articles, of which 12 were wrongly decoded. Those articles were discarded. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8f777a-0a6d-4b58-9c29-9e5b8b862763",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>citations</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Continual Reinforcement Learning with TELLA</td>\n",
       "      <td>http://arxiv.org/pdf/2208.04287v1</td>\n",
       "      <td>2</td>\n",
       "      <td>Workshop Track - 1st Conference on Lifelong Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An exact mapping between the Variational Renor...</td>\n",
       "      <td>http://arxiv.org/pdf/1410.3831v1</td>\n",
       "      <td>295</td>\n",
       "      <td>arXiv:1410.3831v1  [stat.ML]  14 Oct 2014An ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Learning Generative Models across Incomparable...</td>\n",
       "      <td>http://arxiv.org/pdf/1905.05461v2</td>\n",
       "      <td>69</td>\n",
       "      <td>Learning Generative Models across Incomparable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On the Generalization Ability of Online Learni...</td>\n",
       "      <td>http://arxiv.org/pdf/1305.2505v1</td>\n",
       "      <td>74</td>\n",
       "      <td>On the Generalization Ability of Online Learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geometric Understanding of Deep Learning</td>\n",
       "      <td>http://arxiv.org/pdf/1805.10451v2</td>\n",
       "      <td>110</td>\n",
       "      <td>Geometric Understanding of Deep Learning\\nNa L...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0        Continual Reinforcement Learning with TELLA   \n",
       "1  An exact mapping between the Variational Renor...   \n",
       "2  Learning Generative Models across Incomparable...   \n",
       "3  On the Generalization Ability of Online Learni...   \n",
       "4           Geometric Understanding of Deep Learning   \n",
       "\n",
       "                                link  citations  \\\n",
       "0  http://arxiv.org/pdf/2208.04287v1          2   \n",
       "1   http://arxiv.org/pdf/1410.3831v1        295   \n",
       "2  http://arxiv.org/pdf/1905.05461v2         69   \n",
       "3   http://arxiv.org/pdf/1305.2505v1         74   \n",
       "4  http://arxiv.org/pdf/1805.10451v2        110   \n",
       "\n",
       "                                                text  \n",
       "0  Workshop Track - 1st Conference on Lifelong Le...  \n",
       "1  arXiv:1410.3831v1  [stat.ML]  14 Oct 2014An ex...  \n",
       "2  Learning Generative Models across Incomparable...  \n",
       "3  On the Generalization Ability of Online Learni...  \n",
       "4  Geometric Understanding of Deep Learning\\nNa L...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59897c16-49a0-4f79-8fd8-6413c0a90f6f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7285b78-6dee-4789-8920-624131d03e0d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- code for data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9df5e2-6017-47be-806f-9d10772a324a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have used Porter Stemmer and Regexp Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfee65c-c746-416d-a0bb-e41b850229d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "do not run the cell below - it is just for presentation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a93bd-72e2-4660-9099-1b9e2e47ff2f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf23efb-2100-49cc-adf8-a54d4faaa7f0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have constructed a lengthy and chaotic function ```preprocess_text```, which (believe us), in order, does this: <br>\n",
    "- [x] Gets rid of whitespace and numbers ```re.sub(r\"[\\s\\d]+\", \" \", word)```\n",
    "- [x] Gets rid of LaTex equations ```re.sub(r\"(\\${1,2})(?:(?!\\1)[\\s\\S])*\\1\", ... ```\n",
    "- [x] Tokenizes the words ``` tokenizer.tokenize(... ```\n",
    "- [x] Gets rid of words that are shorter than 2 characters ``` if len(word) > 2 ```\n",
    "- [x] Gets rid of \"special\" words we have identified as useless ```word not in [ ... ] ```\n",
    "- [x] Stems the result of it all\n",
    "\n",
    "Finally, we have applied CountVectorizer to the output.\n",
    "- [x] Using max_df and min_df we have gotten rid of too rare or too frequent words\n",
    "- [x] Using stop_words='english' we have gotten rid of english stopwords\n",
    "- [x] We have extracted word ngrams in the boundaries of (1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93ea27-c20b-4018-ae8e-68ef86503f72",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "do not run the cell below - it is just for presentation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817948f7-02b3-451d-8ca9-95a08cd21604",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text: str):\n",
    "    return \" \".join(\n",
    "        [\n",
    "            stemmer.stem(word)\n",
    "            if len(word) > 2\n",
    "            and word\n",
    "            not in [\n",
    "                \"uni\",\n",
    "                \"uni uni\",\n",
    "                \"uni uni uni\",\n",
    "                \"ieee\",\n",
    "                \"doi\",\n",
    "                \"vextendsingl\",\n",
    "                \"http\",\n",
    "                \"https\",\n",
    "                \"vextenddoubl\",\n",
    "                \"parenrightbig\",\n",
    "                \"parenleftbig\",\n",
    "            ]\n",
    "            else \"\"\n",
    "            for word in tokenizer.tokenize(\n",
    "                \" \".join(\n",
    "                    [\n",
    "                        re.sub(\n",
    "                            r\"(\\${1,2})(?:(?!\\1)[\\s\\S])*\\1\",\n",
    "                            \" \",\n",
    "                            re.sub(r\"[\\s\\d]+\", \" \", word),\n",
    "                        )\n",
    "                        for word in text.split()\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "tf_vectorizer = CountVectorizer(ngram_range = (1, 4),\n",
    "                                max_df = 0.8,\n",
    "                                min_df = 0.01,\n",
    "                                tokenizer = tokenizer.tokenize,\n",
    "                                stop_words='english'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd81502-8e0c-4460-9c1e-0a4f130ff02e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Our fitted topic model is available in the model.pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20e47c85-7b70-405e-bf52-eddbb99d9a1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(learning_decay=0.6, learning_offset=30, max_iter=150,\n",
       "                          mean_change_tol=0.01, n_components=5, n_jobs=-1,\n",
       "                          verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_decay=0.6, learning_offset=30, max_iter=150,\n",
       "                          mean_change_tol=0.01, n_components=5, n_jobs=-1,\n",
       "                          verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(learning_decay=0.6, learning_offset=30, max_iter=150,\n",
       "                          mean_change_tol=0.01, n_components=5, n_jobs=-1,\n",
       "                          verbose=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "lda = pickle.load(open('model.pkl', 'rb'))\n",
    "lda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531fa647-7e7b-4dee-9d3a-9b392f4a685c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Presentation and interpretation of results\n",
    "Aggregated Profiles plot:\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/jzmujdzin/topic-modelling-citation-prediction/main/agg_profiles_static.png\">\n",
    "<br>\n",
    "Interactive plot (html link):\n",
    "<a href=\"agg_profiles.html\" target=\"_blank\">here</a>\n",
    "<br> <br>\n",
    "Interpretation of Aggregated Profiles\n",
    "Interestingly, for all topics, the high values of citations were when certain topic probability was very close to 0. They sharply decreased then and slowly increased as probability of topic rose. This is due to the fact that usually the probability that the paper was relevant to a topic was either really low or really high.\n",
    "Interestingly, 2 topic stood out when it comes to citations: Topic 0 (Cybersecurity/Hacking/Reinforcement Learning) and Topic 4 (Classification/Modelling). Topic 1 and 2 were very similar in citations. Topic 3 (Reinforcement Learning) interestingly had similar number of citations throughout the whole p, though had one outlier at approx. p = 0.72, where it reached 600 citations.\n",
    "The conclusions from Aggregate Profiles analysis are not that detached from reality. Overall, Cybersecurity can be considered as \"trendy\" topic over the last few years, while the Classification/Modelling topic is so broad it is not surprising it collects so much citations.\n",
    "<br> <br>\n",
    "Variable importance plot:\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/jzmujdzin/topic-modelling-citation-prediction/main/var_importance_static.png\">\n",
    "<br>\n",
    "Interactive plot (html link):\n",
    "<a href=\"var_importance.html\" target=\"_blank\">here</a>\n",
    "<br> <br>\n",
    "Interpretation of Variable Importance\n",
    "Topic 0 (Cybersecurity/Hacking/Reinforcement Learning) had the highest drop-out loss metric. The model considered the variable as providing the most information from all other variables. Interestingly, Topics 1, 4 and 2 had very similar values of drop-out loss and they can be considered as equally worth keeping from the perspective of XGB Regressor model. Topic 3 (Reinforcement Learning) was the least valuable in terms of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e42edb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tomotopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtomotopy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tomotopy'"
     ]
    }
   ],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "mdl = tp.SLDAModel.load('best_model.bin')\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31937334",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sld = pd.read_csv(\"slda_results_2.csv\").drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca8de93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>min_df</th>\n",
       "      <th>rm_top</th>\n",
       "      <th>vars</th>\n",
       "      <th>alpha</th>\n",
       "      <th>eta</th>\n",
       "      <th>mu</th>\n",
       "      <th>nu_sq</th>\n",
       "      <th>glm_param</th>\n",
       "      <th>seed</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>l</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0.848175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>l</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1.054079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>l</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>25.392506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>l</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>74.194412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>l</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>103.041735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>l</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>112.171611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>l</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>122.279371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>l</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>136.615933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>l</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>145.060881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>l</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>291.991895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>l</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>327.507509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>l</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>357.279043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>l</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>365.608239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>l</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>365.621393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>l</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>365.773638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>l</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>380.574457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>l</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>390.572526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>l</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>395.134499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>l</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>396.944743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>l</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>399.140367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>l</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>399.160618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>l</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>401.554314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>l</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>409.051035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>l</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>409.121243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>l</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>410.158903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>l</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>410.345526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>l</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>411.168225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>l</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>413.598667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>l</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>415.703519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>l</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>415.735967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k  min_df  rm_top vars  alpha   eta   mu  nu_sq  glm_param  seed  \\\n",
       "23  25       0       5    l    0.2  0.01  0.0      1          1   123   \n",
       "15  25       0       0    l    0.3  0.01  0.0      1          1   123   \n",
       "4   20       0       5    l    0.1  0.01  0.0      1          1   123   \n",
       "3   20       0       5    l    0.3  0.01  0.2      1          1   123   \n",
       "8   15       0       1    l    0.3  0.01  0.2      1          1   123   \n",
       "6   20       0       1    l    0.2  0.01  0.0      1          1   123   \n",
       "29  15       0       1    l    0.1  0.01  0.1      1          1   123   \n",
       "12  15       0       0    l    0.2  0.01  0.2      1          1   123   \n",
       "24  15       0       5    l    0.1  0.01  0.0      1          1   123   \n",
       "9   25       0       2    l    0.3  0.05  0.2      1          1   123   \n",
       "20  25       0       0    l    0.1  0.10  0.0      1          1   123   \n",
       "22  15       0       1    l    0.3  0.10  0.1      1          1   123   \n",
       "21   5       0       0    l    0.1  0.05  0.2      1          1   123   \n",
       "10   5       0       0    l    0.3  0.05  0.1      1          1   123   \n",
       "26  20       0       0    l    0.3  0.10  0.1      1          1   123   \n",
       "5   25       0       1    l    0.3  0.20  0.2      1          1   123   \n",
       "14  25       0       2    l    0.2  0.10  0.2      1          1   123   \n",
       "13  10       0       0    l    0.3  0.05  0.2      1          1   123   \n",
       "27  10       0       2    l    0.1  0.05  0.1      1          1   123   \n",
       "28   5       0       1    l    0.1  0.01  0.1      1          1   123   \n",
       "18   5       0       1    l    0.2  0.01  0.2      1          1   123   \n",
       "0   20       0       0    l    0.3  0.20  0.2      1          1   123   \n",
       "11  20       0       5    l    0.3  0.10  0.2      1          1   123   \n",
       "25  20       0       1    l    0.1  0.20  0.1      1          1   123   \n",
       "17  15       0       1    l    0.1  0.20  0.0      1          1   123   \n",
       "2   10       0       5    l    0.3  0.05  0.1      1          1   123   \n",
       "1   20       0       5    l    0.1  0.10  0.1      1          1   123   \n",
       "16  15       0       5    l    0.2  0.20  0.0      1          1   123   \n",
       "19   5       0       2    l    0.1  0.20  0.0      1          1   123   \n",
       "7    5       0       0    l    0.2  0.20  0.0      1          1   123   \n",
       "\n",
       "          mape  \n",
       "23    0.848175  \n",
       "15    1.054079  \n",
       "4    25.392506  \n",
       "3    74.194412  \n",
       "8   103.041735  \n",
       "6   112.171611  \n",
       "29  122.279371  \n",
       "12  136.615933  \n",
       "24  145.060881  \n",
       "9   291.991895  \n",
       "20  327.507509  \n",
       "22  357.279043  \n",
       "21  365.608239  \n",
       "10  365.621393  \n",
       "26  365.773638  \n",
       "5   380.574457  \n",
       "14  390.572526  \n",
       "13  395.134499  \n",
       "27  396.944743  \n",
       "28  399.140367  \n",
       "18  399.160618  \n",
       "0   401.554314  \n",
       "11  409.051035  \n",
       "25  409.121243  \n",
       "17  410.158903  \n",
       "2   410.345526  \n",
       "1   411.168225  \n",
       "16  413.598667  \n",
       "19  415.703519  \n",
       "7   415.735967  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sld.sort_values(by=\"mape\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fadf26",
   "metadata": {},
   "source": [
    "#### sLDA results intepretation\n",
    "As we can see, there is a significant difference between the first two models and the rest of them. Their mean absolute percentage error is reasonable and allows to determine whether the paper is going to be famous or not.  \n",
    "\n",
    "#### List of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINTED_TOPICS = 5\n",
    "topic_list = []\n",
    "for i in range(PRINTED_TOPICS):\n",
    "    topic_list.append(mdl.get_topic_words(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d7ab6-108f-49b7-a0bd-f82f46a63d4a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusions\n",
    "In this research we tried to verify the following hypothesis: \n",
    "#### How can the text content of a paper be used to predict its citation count?  \n",
    "We can use topic modelling algorithms to predict citation counts. Yet, we need to choose algorithms supervised versions or use a combination of two different models to be able to predict the value of the citations from the text.\n",
    "\n",
    "#### Can machine learning algorithms be trained to accurately predict the citation count of a paper based on its text content?\n",
    "We have shown in our study that machine learning algorithms can be trained to predict the citation count of a paper based on its text content. The models are totally accurate, yet their predictive abilities allow us to see whether the paper will be cited a lot or not.\n",
    "\n",
    "#### Can combination of the tasks of topic modelling and citation prediction produce better results than when the tasks are separated?\n",
    "Our study has shown that the sLDA model outperformed combining LDA with XGBoost based on the MAPE metrics. Yet, the difference was not tha big and sLDA model has its downsides - computational cost of its training it's much bigger, then LDA and XGBoost.\n",
    "\n",
    "In this research, the hypothesis \"How can the text content of a paper be used to predict its citation count?\" was tested. The results showed that text content can indeed be used to predict the citation count of a paper. Machine learning algorithms are capable of accurately predicting the citation count based on the text content, providing valuable insights into the potential impact and recognition of a paper.\n",
    "\n",
    "It was also found that combining the tasks of topic modeling and citation prediction can produce better results than when the tasks are performed separately. The sLDA model was found to outperform the combination of LDA and XGBoost based on the MAPE metrics, indicating that sLDA is a more effective model for this particular task. However, it is important to note that the results may vary depending on the specific application and the data being used.\n",
    "\n",
    "While the results from this study are encouraging, it is important to remember that there are limitations to the accuracy of machine learning algorithms in predicting the citation count. There are numerous factors that can influence the citation count of a paper, including the quality of the research, the relevance of the topic, and the visibility of the paper. These factors cannot be solely captured by the text content of the paper, and therefore, the results of this research should be considered in conjunction with other relevant information.\n",
    "\n",
    "In conclusion, the findings from this research highlight the potential of text content to predict the citation count of a paper and the role of machine learning algorithms in this process. Further research is needed to improve the accuracy and applicability of these models, and to better understand the various factors that influence the citation count of a paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('3.10.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "55729c9ac2c7dbad6f6d23b8a61c7691f855a2bc34416b74858e7ea5b72a26bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
